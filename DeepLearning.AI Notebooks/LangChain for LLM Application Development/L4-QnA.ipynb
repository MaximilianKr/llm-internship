{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ee82f4-bb88-41d3-b1d8-f2bcf3b2fb0b",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f0108-a073-4035-bd3d-63a705b242b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EITHER: use your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee7115f-c680-4d59-97bb-8737fea9c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8cbe6-3601-4d12-8ad9-cd0d43e7092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fb75c-a52b-48f0-9386-324e7bdef67c",
   "metadata": {},
   "source": [
    "## OR: use [LocalAI as an OpenAI replacement](https://localai.io/howtos/easy-request-openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a624f3ab-c768-469b-bc4d-9a8f84e58ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Specify the port your LocalAI docker container runs on\n",
    "# openai.api_base = \"http://localhost:8080/v1\"  # default\n",
    "openai.api_base = \"http://localhost:9095/v1\"  # for lunademo\n",
    "openai.api_key = \"sx-xxx\"  # not needed for LocalAI (dummy)\n",
    "OPENAI_API_KEY = \"sx-xxx\"\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6521eb-8cb3-44ee-810f-d2ba2bf84ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model you are using\n",
    "# llm_model = \"\"\n",
    "llm_model = \"lunademo\"  # for lunademo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7cffc-8ecf-4404-8274-39eb20968e13",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc5f47-54bb-496f-ab24-4d2b3ade4437",
   "metadata": {},
   "source": [
    "**Components we need:**\n",
    "- **RetrievalQA Chain** - for retrievel over some documents\n",
    "- **ChatOpenAI** --> *could we swap this with local Llama?*\n",
    "- **CSVLoader** - one of many different document loaders\n",
    "- **DocArrayInMemorySearch** - one of many different types of **vector stores**, this one is *in-memory* vector store, does not connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c1b1a-c4ae-436d-860a-3c41acf3f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7cc6be-1bb2-4c7b-9941-64ccd0b6c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown  # to display the response in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a8d7c2d-bd08-46ab-b0eb-5ff10a6407ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f8531-f95d-43b5-b547-d2f87aa4bb5b",
   "metadata": {},
   "source": [
    "**- VectorstoreIndexCreator** - to create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c70f1-de5f-4a39-bebf-562df477a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8eede24-ef8c-40d4-b21f-64da4a9323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205541b-e71e-4781-b307-d98691d60bee",
   "metadata": {},
   "source": [
    "**Specify the vector store class (we use *DocArrayInMemorySearch* imported before) and load a list of loaders (here only one)**\n",
    "- **FixMe:** embeddings do not work with LocalAI `lunademo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3df352-d3e9-47a2-9335-9e38df3e3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840e171-c2ac-4e2d-ae6d-beb009e492e6",
   "metadata": {},
   "source": [
    "**This is the query we ask our loaded document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffaf3d-1602-4606-80a8-d122bb432c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766cca5-6b11-481b-8817-c0b4498f883a",
   "metadata": {},
   "source": [
    "**Create the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66dab1-c402-4daf-b3f1-b3c190e4c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5c1a3-7453-4b5b-b5ea-6696c1ec8666",
   "metadata": {},
   "source": [
    "**Display the response in markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dac705-dd2a-4291-aae2-05c219c7c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))  # will display a table with different product names and their descriptions, as well as a short text summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b086db-a0ea-4570-952e-b12bc985ca4f",
   "metadata": {},
   "source": [
    "## Step By Step\n",
    "- more in depth creation of the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2659afc-5e4c-4d84-9605-713459ce18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665ccdd3-ddd1-41e1-8489-757598db0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e6fd5-f5cc-48a4-97c4-e4fe38f19ca1",
   "metadata": {},
   "source": [
    "**If we look at the individual documents, we see that each document corresponds to one product in the original csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07383611-8e07-4fec-9325-9a6ca877f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXTÂ® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a153804-b8ef-409a-be35-80c60d85ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14bd8c8-debc-438b-ae91-c1f49fddd3d2",
   "metadata": {},
   "source": [
    "**We use OpenAI embeddings** (but there are alternatives)\n",
    "- because the documents are so small, we don't need to do any *chunking* --> for larger documents this is necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf7095-ad5c-4d46-8bc8-785b93ff30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d23cb8-3363-4e88-a6d3-fb0d06e252bf",
   "metadata": {},
   "source": [
    "**Embedding of an example string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e9e4e-12fe-414a-82b1-e9d22b054cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi my name is Max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c67ac-fd68-4016-8a37-5267c110b12c",
   "metadata": {},
   "source": [
    "**Will give us an embedding vector with 1536 elements --> the overall numerical representation of this piece of text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4445f-cb83-4bff-a79b-bb759ec5e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405701df-8b67-49f5-8d6e-cb83113060c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4e779-7002-437a-b552-ef0cfe359c4b",
   "metadata": {},
   "source": [
    "**Store the embeddings in the vector store** \n",
    "- using *from_documents()* function\n",
    "- takes list of documents + embeddings object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b352c-ad07-4c30-b279-56c529d6597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba490af-782c-4ea4-98c0-5327ae81af9f",
   "metadata": {},
   "source": [
    "**We write a new query to check it against the vector store to find similar pieces of text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98216578-9515-4bf0-a3b1-95ca9f401a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97eeeb1-3c30-411f-8993-2229aa0b0b56",
   "metadata": {},
   "source": [
    "**Will return a list of documents (here: 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ebbc5-881f-453f-bf7f-5cc5ab915b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a6786-c5e6-414c-a72b-1cc12b99a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5008544-5a95-47b3-90b6-be599605b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b6932-342b-4df8-bce0-caa65193ae63",
   "metadata": {},
   "source": [
    "### Now we use this to do Q&A over our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d82c86-4031-416a-b0ad-2ed738acb09a",
   "metadata": {},
   "source": [
    "**We first need a retriever from our vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9e747-520e-45bd-8913-e0de56ef6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1455a00-fb56-4c17-86ae-9ba5774ffa86",
   "metadata": {},
   "source": [
    "**We want to do text generation for a natural language response (here we use OpenAI again)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f6436-8257-486c-9bde-de4ffbbf3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48853fb2-168f-44a6-a194-606c74760e75",
   "metadata": {},
   "source": [
    "**If we did this by hand, we would combine all documents into a single piece of text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41915a5e-d1e3-4c44-9a39-779dbd10d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2b7f8-2e48-4200-ba6e-d44c60c81d00",
   "metadata": {},
   "source": [
    "**And ask the LLM to format it to a summarized table for us**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea72e3-aa51-4ade-815a-d833c12e1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21601fe8-5b35-423f-9bbd-75f090ec9520",
   "metadata": {},
   "source": [
    "**Get the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580642b-e49f-46dc-a9c0-1f55a8f4bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522ab4b-d384-4a94-b15a-1c4321573f7e",
   "metadata": {},
   "source": [
    "## We can do all this with a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2c298-be59-4b8e-8ebf-d32d4003768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e4e5d-9ba1-4f4c-879c-d57ddca774f8",
   "metadata": {},
   "source": [
    "**We create a retrieval QA chain**\n",
    "- does Q&A over the retrieved documents\n",
    "- we pass an LLM (for text generation in the end)\n",
    "- a chain type, here *stuff*\n",
    "- a retriever (the interface for fetching documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1f616-60e3-4bd6-abd0-cc038ff30695",
   "metadata": {},
   "source": [
    "**Stuff method:**\n",
    "- simplest but most popular method, *stuffs* all documents into one prompt and make single call to LLM\n",
    "    - LLM has access to all data at once\n",
    "    - but keep context lenght of LLM in mind, will not work for large documents or many documents at one\n",
    " \n",
    "**Other methods**\n",
    "- *map_reduce* calls LLM for every chunk, combines all responses into another call to LLM, get final answer\n",
    "    - treats all documents as independent, can be done in parallel --> fast\n",
    "    - but context across documents might be lost\n",
    "    - most often for **summarization**\n",
    "- *refine* iteratively call LLM for every chunk\n",
    "    - build upon answer of previous document, really powerful for combining information and building up answer over time\n",
    "    - will lead to longer answers --> slow\n",
    "- *map_rerank* calls LLM for every chunk and returns a score, then selects the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1393e-b9e5-4181-91f1-a55c3b345e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23c45e-734b-4dcb-89ad-a36e23f86a86",
   "metadata": {},
   "source": [
    "**We create a new query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14355349-5fc3-4e38-ae05-dd599240ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348cf16-b539-450e-b9c4-1d18db1daab6",
   "metadata": {},
   "source": [
    "**And run our created chain on the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727ae54-6fd0-4113-b646-2d6c25c5578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b54c0-149d-4874-8930-47d180e67cd2",
   "metadata": {},
   "source": [
    "**Display the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50464d65-c194-45d9-9a72-d412a11be64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109416a1-30db-4648-85c9-77c8f9505617",
   "metadata": {},
   "source": [
    "**In one line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e44e3e-5447-447b-8307-040d26d5f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1a84a-22af-4acb-a0e1-53dbd8660e18",
   "metadata": {},
   "source": [
    "**We can also customize the index when we're creating it**\n",
    "- e.g., specify the embeddings, swap out the vector store for a different type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40211b1-265a-43d3-a545-7d1e0cb50781",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f64839-353c-43bc-90ed-e925767e0019",
   "metadata": {},
   "source": [
    "### Source: https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
