{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da59f65e-14da-4c00-bb57-585c92e37f50",
   "metadata": {},
   "source": [
    "# LangChain: Evaluation\n",
    "- useful when comparing different strategies\n",
    "    - swapping model\n",
    "    - using different vector database\n",
    "    - how to create chunks, etc.\n",
    "- **LangSmith**?\n",
    "\n",
    "## Outline:\n",
    "\n",
    "* Example generation\n",
    "* Manual evaluation (and debuging)\n",
    "* LLM-assisted evaluation\n",
    "* LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa9f3d-17fd-4d03-bf6c-be682a9ffcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a64e8c-1750-4109-beb3-2c8f3eea97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1721b-1c60-4ba6-99af-dcbd787e462f",
   "metadata": {},
   "source": [
    "## Create our QandA application\n",
    "- we're using the same Chain we build in the previous session on Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39770c0-d9a1-4a4d-ae27-b51f33df4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567759c3-8e78-401c-aee6-13eb6e17188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9cf93-c42f-44fb-8cea-131f7b007e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513bded-6bce-4ef2-a15f-a6c33bee5b94",
   "metadata": {},
   "source": [
    "**Specify the LLM and Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dd677-ee3a-40af-be02-5a2810b7160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, model=llm_model)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73052ed9-2070-4b85-973f-9a1707658845",
   "metadata": {},
   "source": [
    "### Coming up with test datapoints\n",
    "- looking of a few of the documents here\n",
    "- coming up with ground truth examples manually --> takes time, does not scale well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb090e-6625-4e65-8da4-6e49bfe19160",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acefe2d-7ff5-4408-9659-de366692dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f77f2-156b-48ec-a98f-e8cf7c8763a3",
   "metadata": {},
   "source": [
    "### Hard-coded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb6024-bbac-4f33-9a19-49cf4121efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd847d-433f-4307-986f-179730f7b733",
   "metadata": {},
   "source": [
    "### LLM-Generated examples\n",
    "- automate the steps above for ground truth examples\n",
    "- we use a specific chain **QAGenerateChain** for that\n",
    "- it takes documents as input and **automatically generates Question/Answer pairs from each document** using an LLM itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd97a5-90d3-41a8-874c-c2d2b2c66f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d208153-5375-4c41-873f-55288a5a0eb8",
   "metadata": {},
   "source": [
    "**Pass the LLM used to generate the Question/Answer pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a9fad-43d3-4876-a3f1-d1608a2a7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b368d22-67f8-4c4f-acdf-68ba0f4ac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the warning below can be safely ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb014971-8f67-44ea-ac9d-f53451db09c5",
   "metadata": {},
   "source": [
    "**Generate examples using apply_and_parse() method**\n",
    "- we want to get back a dict which has *query: answer* pair entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16060d23-509e-4d3f-bf6d-cef738f705ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7e483-dd42-4666-86d6-48e600301612",
   "metadata": {},
   "source": [
    "### Combine examples\n",
    "- add the generated examples to the ones we already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90993b91-7ad8-4fa1-b114-e0db1f7badd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples += new_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd7674-c420-46e2-bec2-27e8ea098f53",
   "metadata": {},
   "source": [
    "**Manual test: run an example through the Q&A chain and get the LLMs response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a350dd8-4837-408d-adf4-e1bc42c34b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8dbbb-91d3-4f47-8c9e-6ad16e2b7ec9",
   "metadata": {},
   "source": [
    "## Manual Evaluation\n",
    "- how do we know what's actually happening inside the chain? Which prompt is used, etc.?\n",
    "- we can use LangChain's *debug*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adafff3-9452-4dd6-9d95-b3f979f77433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05aea7-744f-4a54-9c3f-ff05a4a9b352",
   "metadata": {},
   "source": [
    "**Manual test again, with debug: run an example through the Q&A chain and get the LLMs response**\n",
    "- we can see it's using the *RetrievalQA* chain first\n",
    "- then enters a *StuffDocumentsChain* (using the *stuff* method)\n",
    "- lastly it uses the *LLMChain* with the original question, passing context from the retrieved document(s)\n",
    "- when things go wrong: often times it's not the LLM messing up, but the retrieval\n",
    "- looking up what the question and the used context is can help with debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d14e87-e79f-41cf-876c-c24205b7e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e209cd-f924-4d49-a56e-ee202147679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66976798-256f-4da7-b69d-f4a98b86efcd",
   "metadata": {},
   "source": [
    "## LLM assisted evaluation\n",
    "- now we are letting an LLM evaluate the predictions to our test examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39d756-ce44-4f48-a305-54f396546c33",
   "metadata": {},
   "source": [
    "**Get predictions for all examples first**\n",
    "- this stept might take a while running through all chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b785de-01be-405f-9601-584bf1c0ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = qa.apply(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e64145-fd3d-4737-b02d-66be15768d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16020b44-7fe8-498b-9a1d-93ed1a16d8e0",
   "metadata": {},
   "source": [
    "- again, we need an LLM to grade so we declare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e2f62-fa1a-48fc-b25c-80093394e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507650c-6d04-4299-8e3f-9bd2f81e2f31",
   "metadata": {},
   "source": [
    "**We use a chain to grade the predictions**\n",
    "- by passing our test examples and the generated predictions to an **QAEvalChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66ae2f-ff95-4ce5-be11-1c50549d02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b31d6-73d8-4895-bf1c-8ead45094c3c",
   "metadata": {},
   "source": [
    "- for each example we are going to loop through them \n",
    "    - print the question (generated by the LLM)\n",
    "    - print the ground truth answer (generated by the same LLM which had the original document as context)\n",
    "    - print the prediction (generated by another LLM using the QAChain, which retrieved the document from the vector database)\n",
    "    - print the grade (generated by yet another LLM taking all the above as context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d7cca-f76b-46a6-98a9-e932b57985ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d70238-9de5-4ea8-8ff4-755c7e4000bb",
   "metadata": {},
   "source": [
    "## LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc31be-5932-4a16-a2af-e3c67462d547",
   "metadata": {},
   "source": [
    "https://smith.langchain.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf17bb-491a-43ef-9187-2a7ec478244a",
   "metadata": {},
   "source": [
    "### Source: https://learn.deeplearning.ai/langchain/lesson/6/evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
