{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285e499b-9b95-4c3a-aed2-5c9a7e8c8827",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "- LLMs are *stateless* = each transaction is independent\n",
    "- Chatbots appear to have memory by providing the full conversation (history) as context\n",
    "\n",
    "- **memory** = how you remember previous parts of a conversation and feed that into the language model so they can have a conversational flow each time you interact with them\n",
    "- memory input is used as additional context so the LLM can generate output like it generating the next conversational turn knowing what's been said before\n",
    "- **HOWEVER**, as conversation becomes long, amount of memory becomes large, thus also **cost** of sending many tokens to the LLM\n",
    "- LangChain provides several ways to store memory and accumulate the conversation\n",
    "\n",
    "## Outline\n",
    "* **ConversationBufferMemory** *(entire conversation from start to end)*\n",
    "* **ConversationBufferWindowMemory** *(specify window for memory, e.g. `k=1` conversational exchanges (=1 user utterance + 1 LLM response)*\n",
    "* **ConversationTokenBufferMemory** *(limit the number of tokens in memory `max_token_limit=x`)*\n",
    "* **ConversationSummaryMemory** *(let the LLM write a summary of the current conversation and use this as memory, specify `max_token_limit=x`)*\n",
    "\n",
    "**Additional memory types:**\n",
    "\n",
    "* **Vector data memory** --> *for storing embeddings*\n",
    "    - stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant block of text\n",
    "* **Entity memories**\n",
    "    - using an LLM, remembers details about specific entities</br></br>\n",
    "\n",
    "- you can use multiple memories at one time, e.g., ConversationMemory + EntityMemory to recall individuals\n",
    "- you can also store the conversation in a conventional database (e.g., key-value store or SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59dfe5-061b-4a60-ac8c-2c44782d4006",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory \n",
    "### *(entire conversation from start to end)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6589c-8c54-47bc-855f-84ad57c05c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033d229-c6d8-4428-953a-941feef88ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2496e8-3e66-4a83-b490-ca4f1944dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77009cdb-93c2-4be4-b7e2-902180abbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature: degree of randomness (default 0.7)\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# set 'verbose=False' to not output the actual chain when running 'conversation.predict()''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2a0a5-2e08-46c2-928e-cc4ee1799794",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Max.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef49eb3-378d-4e98-b0c0-d765be601bca",
   "metadata": {},
   "source": [
    "**Probably gets this one wrong. But you can run it repeatedly and see that the response changes, even if the value stays wrong.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffc1c4-f988-451f-8b51-03fb552847c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is the sum of digits of 111111911111?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99518dbd-6aa8-41f5-b9e0-9f3203ec06cd",
   "metadata": {},
   "source": [
    "**It will remember your name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3f879-6065-4edb-8fa2-f6d005e4f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78382052-a175-4e6a-93be-3b39ff9a16ba",
   "metadata": {},
   "source": [
    "**View the chat history (from the `memory` variable declared above)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99d905-5b68-4455-894e-5eb8f43130c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f3bb2-afd9-49b5-b15a-8087eb8c0b5e",
   "metadata": {},
   "source": [
    "**Load memory variables**\n",
    " - *notice the empty `dict()` as input for the function (the output of the function will be stored there)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacd35d-c147-4314-b256-0b8cfa68b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "# memory.load_memory_variables(dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ed245-0253-42b1-9f4a-9748f79597e9",
   "metadata": {},
   "source": [
    "**You can explicitly add things to the memory, if you wish to do so**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ae80cf-822a-4c51-ac5c-6eaebdfd5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507b3c8a-17e8-4483-b374-bca4521d929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98420e8b-216d-45b5-89c9-388a01d72a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795a4126-8cb2-4f05-97e0-528f93a817b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a3adebb-e35e-4ffd-840d-8bccdf6eef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4e4b72-ccb0-4b06-ae24-f9dd23c2413a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93a187-e896-4e08-9b27-bb14c6f8c002",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "\n",
    "### *(specify window for memory, e.g. `k=1` conversational exchanges ( = 1 user utterance + 1 LLM response)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296b3b48-c852-4883-9ce3-f2be9dee7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f50107e-a1b2-4979-9fa2-af1485e7346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55034e1c-a499-4229-86bd-23d817003ea5",
   "metadata": {},
   "source": [
    "**Try to save `k=2` conversational exchanges to memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb04425-e138-4dfc-9285-823d48f9079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c27c7-18ab-4d84-a3c8-6745619b5bea",
   "metadata": {},
   "source": [
    "**Only the last exchange `k=1` will be remembered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4620a2e7-2fdd-4293-99e6-af0c553ceba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc4239-afca-4d9c-ae9c-4fd804f5e35a",
   "metadata": {},
   "source": [
    "**Try the same conversation from before again, but this time with `k=1` window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e1293-de8f-4104-b6e9-8330a17143ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# try this with 'verbose=True' to see the actual chain output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f4251-c6d6-452c-a6d1-6c9c3a6ff377",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd13ef-b289-43a1-96d3-98f6b8fc5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bb372-35f5-41f0-9e5c-cc91cf8f2e43",
   "metadata": {},
   "source": [
    "**This time, it won't remember your name anymore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762008a-e523-48bc-afba-aa56675cfd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aaa15-77d2-41f1-803d-aa0c1949d40a",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "### *(limit the number of tokens in memory `max_token_limit=x`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0003b-3fa6-4754-b345-ab62273daa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f3c84-65a2-4e20-963e-0ce2f8a564cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7155a-ce44-4633-9c96-5aa666e8b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=25)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d29d0a-d2f1-437b-9d86-b1c99cfb1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75314690-d4cc-4c3b-a97f-5c5cc444108f",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "### *(let the LLM write a summary of the current conversation and use this as memory)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e4db9a4-9893-41e9-a991-1ebe9b3ba00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134d314-048b-4fac-af68-e10a63ec6ead",
   "metadata": {},
   "source": [
    "**Example for a long string (for a long conversation)**\n",
    "- **Specify `llm=llm` as different LLMs use different ways of counting tokens**\n",
    "- **Specify `max_token_limit=x`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616abdd-83c3-40e7-9cfd-799bb3b10b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708b711-5cf4-4c32-a69c-442f72a126ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c8666-fb19-4f10-ba04-a311ac90ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595db19-a3ac-4757-ad2c-166327da416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce11dc-e299-4fe1-8aee-2378c0462692",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e68b33-0da4-41d4-b046-00e22ee9a5bf",
   "metadata": {},
   "source": [
    "### Source: https://learn.deeplearning.ai/langchain/lesson/2/models,-prompts-and-parsers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
